# src/train.py (ìµœì¢… í›ˆë ¨ìš© ê¹¨ë—í•œ ë²„ì „)
import os
import sys
import argparse
from omegaconf import OmegaConf
import torch
import pytorch_lightning as pl

# ê²½ë¡œì™€ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.data_module import SummaryDataModule
from src.model_module import SummaryModelModule

def train(cfg):
    # ê¸°ë³¸ ì„¤ì •
    torch.set_float32_matmul_precision('high')
    pl.seed_everything(cfg.seed)
    
    # ëª¨ë“ˆ ë° ë¡œê±° ì´ˆê¸°í™”
    wandb_logger = pl.loggers.WandbLogger(project=cfg.wandb.project)
    data_module = SummaryDataModule(cfg.data, cfg.model)
    model_module = SummaryModelModule(cfg.model, data_module.tokenizer)

    # ì½œë°± ì„¤ì •
    callbacks = []
    for callback_conf in cfg.trainer.callbacks:
        if callback_conf["_target_"] == "pytorch_lightning.callbacks.ModelCheckpoint":
            conf = {k:v for k,v in callback_conf.items() if k != '_target_'}
            callbacks.append(pl.callbacks.ModelCheckpoint(**conf))
        elif callback_conf["_target_"] == "pytorch_lightning.callbacks.EarlyStopping":
            conf = {k:v for k,v in callback_conf.items() if k != '_target_'}
            callbacks.append(pl.callbacks.EarlyStopping(**conf))

    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    trainer_args = {k:v for k,v in cfg.trainer.items() if k != '_target_' and k != 'callbacks'}
    trainer = pl.Trainer(logger=wandb_logger, callbacks=callbacks, **trainer_args)
    
    # special_tokens ì„¤ì •ì´ ìˆì„ ë•Œë§Œ resize ì‹¤í–‰ (í˜„ì¬ëŠ” nullì´ë¼ ì‹¤í–‰ ì•ˆ ë¨)
    if cfg.model.get("special_tokens"):
        model_module.model.resize_token_embeddings(len(data_module.tokenizer))
    
    wandb_logger.watch(model_module, log="all", log_freq=500)
    
    # í›ˆë ¨ ì‹œì‘
    print(f"ğŸš€ [Trainer] ìµœì¢… í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤... ëª¨ë¸: {cfg.model.name}")
    trainer.fit(model=model_module, datamodule=data_module)
    print("âœ… [Trainer] ìµœì¢… í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, required=True, help="e.g., flan-t5-large-final")
    args, unknown_args = parser.parse_known_args()

    # ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ë³‘í•©
    cfg = OmegaConf.load('configs/config.yaml')
    model_cfg = OmegaConf.load(f'configs/model/{args.model_name}.yaml')
    trainer_cfg = OmegaConf.load(f"configs/trainer/default.yaml")
    data_cfg = OmegaConf.load(f"configs/data/default.yaml")
    cfg.merge_with({'model': model_cfg, 'trainer': trainer_cfg, 'data': data_cfg})
    
    # ì»¤ë§¨ë“œ ë¼ì¸ ì˜¤ë²„ë¼ì´ë“œ ì ìš©
    if unknown_args:
        cli_cfg = OmegaConf.from_dotlist(unknown_args)
        cfg.merge_with(cli_cfg)
        
    train(cfg)
    
# # src/train.py (Hydra ì—†ëŠ” ìµœì¢… ë²„ì „)
# import os
# import sys
# import argparse
# from omegaconf import OmegaConf
# import torch
# import pytorch_lightning as pl

# # ìƒëŒ€ ê²½ë¡œ importë¥¼ ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# from src.data_module import SummaryDataModule
# from src.model_module import SummaryModelModule

# def train(cfg):
#     torch.set_float32_matmul_precision('high')
#     pl.seed_everything(cfg.seed)
#     data_module = SummaryDataModule(cfg.data, cfg.model)
#     model_module = SummaryModelModule(cfg.model, data_module.tokenizer)

#     # wandb ë¡œê±° ì„¤ì •
#     wandb_conf = {k: v for k, v in cfg.wandb.items() if k != '_target_'}
#     wandb_logger = pl.loggers.WandbLogger(**wandb_conf)

#     # ì½œë°± ì„¤ì •
#     callbacks = []
#     for callback_conf in cfg.trainer.callbacks:
#         target_class = callback_conf["_target_"]
#         conf = {k:v for k,v in callback_conf.items() if k != '_target_'}
#         if target_class == "pytorch_lightning.callbacks.ModelCheckpoint":
#             # dirpath í¬ë§·íŒ… ì ìš©
#             conf["dirpath"] = conf["dirpath"].format(model_name=cfg.model.name)
#             callbacks.append(pl.callbacks.ModelCheckpoint(**conf))
#         elif target_class == "pytorch_lightning.callbacks.EarlyStopping":
#             callbacks.append(pl.callbacks.EarlyStopping(**conf))

#     # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
#     trainer_args = {k:v for k,v in cfg.trainer.items() if k != '_target_' and k != 'callbacks'}
#     trainer = pl.Trainer(logger=wandb_logger, callbacks=callbacks, **trainer_args)

#     model_module.model.resize_token_embeddings(len(data_module.tokenizer))
#     wandb_logger.watch(model_module, log="all", log_freq=500)

#     print("ğŸš€ [Trainer] í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
#     trainer.fit(model=model_module, datamodule=data_module)
#     print("âœ… [Trainer] í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--model_name", type=str, required=True, help="e.g., flan-t5")
#     args = parser.parse_args()

#     # ì„¤ì • íŒŒì¼ ìˆ˜ë™ ë¡œë“œ ë° ë³‘í•©
#     cfg = OmegaConf.load('configs/config.yaml')
#     model_cfg = OmegaConf.load(f'configs/model/{args.model_name}.yaml')
#     trainer_cfg = OmegaConf.load(f"configs/trainer/default.yaml")
#     data_cfg = OmegaConf.load(f"configs/data/default.yaml")

#     # model, trainer, data ì„¤ì •ì„ ê¸°ë³¸ cfgì— ë³‘í•©
#     cfg.merge_with({'model': model_cfg, 'trainer': trainer_cfg, 'data': data_cfg})

#     train(cfg)
# src/train.py (Sweep ì „ìš© ìµœì¢… ë²„ì „)
# import os
# import sys
# from omegaconf import OmegaConf
# import torch
# import pytorch_lightning as pl
# import wandb

# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# from src.data_module import SummaryDataModule
# from src.model_module import SummaryModelModule

# def train(cfg):
#     wandb_logger = pl.loggers.WandbLogger(project=cfg.wandb.project)
#     torch.set_float32_matmul_precision('high')
#     pl.seed_everything(cfg.seed)
#     data_module = SummaryDataModule(cfg.data, cfg.model)
#     model_module = SummaryModelModule(cfg.model, data_module.tokenizer)
#     callbacks = []
#     for callback_conf in cfg.trainer.callbacks:
#         if callback_conf["_target_"] == "pytorch_lightning.callbacks.ModelCheckpoint":
#             conf = {k:v for k,v in callback_conf.items() if k != '_target_'}
#             callbacks.append(pl.callbacks.ModelCheckpoint(**conf))
#         elif callback_conf["_target_"] == "pytorch_lightning.callbacks.EarlyStopping":
#             conf = {k:v for k,v in callback_conf.items() if k != '_target_'}
#             callbacks.append(pl.callbacks.EarlyStopping(**conf))
#     trainer_args = {k:v for k,v in cfg.trainer.items() if k != '_target_' and k != 'callbacks'}
#     trainer = pl.Trainer(logger=wandb_logger, callbacks=callbacks, **trainer_args)
#     if cfg.model.get("special_tokens"): # special_tokens ì„¤ì •ì´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
#         model_module.model.resize_token_embeddings(len(data_module.tokenizer))
#     wandb_logger.watch(model_module, log="all", log_freq=500)
#     print("ğŸš€ [Trainer] í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
#     trainer.fit(model=model_module, datamodule=data_module)
#     print("âœ… [Trainer] í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# if __name__ == "__main__":
#     wandb.init()
#     sweep_config = wandb.config
#     cfg = OmegaConf.load('configs/config.yaml')
#     model_cfg = OmegaConf.load(f'configs/model/{sweep_config.model_name}.yaml')
#     trainer_cfg = OmegaConf.load(f"configs/trainer/default.yaml")
#     data_cfg = OmegaConf.load(sweep_config.data_config_path)
#     model_cfg.learning_rate = sweep_config.learning_rate
#     if hasattr(sweep_config, 'weight_decay'):
#         model_cfg.weight_decay = sweep_config.weight_decay
#     if hasattr(sweep_config, 'label_smoothing'):
#         model_cfg.label_smoothing = sweep_config.label_smoothing
#     trainer_cfg.max_epochs = sweep_config.max_epochs
#     cfg.merge_with({'model': model_cfg, 'trainer': trainer_cfg, 'data': data_cfg})
#     train(cfg)