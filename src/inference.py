# src/inference.py (ì£¼ì„ ì œê±° ë° ëª¨ë“  ê¸°ëŠ¥ í†µí•© ìµœì¢… ë²„ì „)
import os
import sys
import time
import glob
import re
import argparse
import pandas as pd
import torch
from omegaconf import OmegaConf
import pytorch_lightning as pl
import evaluate
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm

# --- 1. ê²½ë¡œ ì„¤ì • ë° API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.data_module import SummaryDataModule
from src.model_module import SummaryModelModule

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")

# Solar API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=UPSTAGE_API_KEY, base_url="https://api.upstage.ai/v1/solar")
print("âœ… Solar API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


# --- 2. API í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜ ---
def api_request_with_rate_limiting(messages):
    try:
        response = client.chat.completions.create(
            model="solar-1-mini-chat", messages=messages, temperature=0.2,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"ğŸ”¥ğŸ”¥ğŸ”¥ API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return ""
    
def translate_texts(texts, prompt_template):
    # 'ko' -> 'en' ì¸ì§€, 'en' -> 'ko' ì¸ì§€ ìë™ ê°ì§€
    direction = "ko -> en" if "Korean" not in prompt_template else "en -> ko"
    print(f"ğŸŒ [API Call] {len(texts)}ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ {direction} ë°©í–¥ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤...")
    
    translated = []
    start_time = time.time()
    
    for i, text in enumerate(tqdm(texts, desc=f"Translating ({direction})")):
        # ì‚¬ìš©ìê°€ ì œê³µí•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¥¼ ì‚½ì…
        prompt = prompt_template.format(text_to_translate=text)
        messages = [{"role": "user", "content": prompt}]
        translated_text = api_request_with_rate_limiting(messages)
        translated.append(translated_text)
        
        # Rate Limiting ë¡œì§ (ì´ì „ê³¼ ë™ì¼)
        if (i + 1) % 100 == 0 and len(texts) > 100:
            end_time = time.time()
            elapsed_time = end_time - start_time
            if elapsed_time < 60:
                wait_time = 60 - elapsed_time + 1
                print(f"  - Rate limit: {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(wait_time)
            start_time = time.time()
            
    return translated

# --- 3. ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜ (ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ì •ì˜ ë° ì ìš©) ---
def inference(cfg):
    pl.seed_everything(cfg.seed)

    data_module = SummaryDataModule(cfg.data, cfg.model)
    best_ckpt_path = find_best_checkpoint(cfg.model.name)
    print(f"âœ… [ëª¨ë¸] ê°€ì¥ ì¢‹ì€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {os.path.basename(best_ckpt_path)}")
    model_module = SummaryModelModule.load_from_checkpoint(
        best_ckpt_path, tokenizer=data_module.tokenizer
    )
    data_module.tokenizer = model_module.tokenizer

    print(f"ğŸš€ [ì¶”ë¡ ] '{cfg.input_path}' íŒŒì¼ì— ëŒ€í•œ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    input_df = pd.read_csv(cfg.input_path)
    
    is_test_run = ('test.csv' in cfg.input_path)
    
    if is_test_run:
        # --- 1ì°¨ ë²ˆì—­ (ko -> en)ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ---
        KO_TO_EN_PROMPT = """
        Translate the following Korean dialogue to natural, fluent English.
        Preserve the speaker tags like #Person1# exactly.

        Korean Dialogue:
        {text_to_translate}

        English Dialogue:
        """
        dialogues_to_process = translate_texts(input_df['dialogue'].tolist(), KO_TO_EN_PROMPT)
    else:
        dialogues_to_process = input_df['input_text'].tolist()

    data_module.predict_df = pd.DataFrame({"input_text": dialogues_to_process})
    data_module.setup('predict')
    
    trainer = pl.Trainer(accelerator="auto", devices=1, logger=False)
    model_module.hparams.generation = cfg.generation
    
    predictions = trainer.predict(model=model_module, dataloaders=data_module.predict_dataloader())
    # english_summaries = [summary for batch in predictions for summary in batch]

    # if is_test_run:
    #     # --- 2ì°¨ ë²ˆì—­ (en -> ko)ì„ ìœ„í•œ ì±”í”¼ì–¸ í”„ë¡¬í”„íŠ¸ (Prompt 3) ---
    #     EN_TO_KO_PROMPT = """
    #     You are a professional translator working on a Korean language dataset for AI training.
    #     Translate the following English summary into **natural, fluent, and detailed Korean**.
    #     **Instructions:**
    #     1. Keep speaker tags such as `#Person1#`, `#person2#` **exactly as they are**.
    #     2. Personal names must be translated **phonetically** (e.g., "Francis" â†’ "í”„ëœì‹œìŠ¤").
    #     3. Use a **formal**, full-sentence tone.
    #     4. DO NOT summarize or skip information. Be as detailed as the source.
    #     ---
    #     Please translate the following English summary:
    #     {text_to_translate}
    #     """
    #     final_summaries = translate_texts(english_summaries, EN_TO_KO_PROMPT)
    # else:
    #     final_summaries = english_summaries


    # submission = pd.DataFrame({'fname': input_df['fname'], 'summary': final_summaries})
    # os.makedirs("submissions", exist_ok=True)
    # submission.to_csv(cfg.output_path, index=False, encoding='utf-8-sig')
    # print(f"âœ… [ì„±ê³µ] ì¶”ë¡  ê²°ê³¼ê°€ '{cfg.output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # src/inference.py ì˜ inference í•¨ìˆ˜ ë‚´ë¶€


    # ... (predictions = trainer.predict(...) ì½”ë“œ ë°”ë¡œ ë‹¤ìŒë¶€í„°)

    all_summaries = [summary for batch in predictions for summary in batch]
    print("âœ… [ì¶”ë¡ ] ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- ê²°ê³¼ ì €ì¥ ë° í‰ê°€ (ë³€ìˆ˜ ì´ë¦„ì„ all_summariesë¡œ ìˆ˜ì •) ---
    input_df = pd.read_csv(cfg.input_path)
    
    # â–¼â–¼â–¼â–¼â–¼ final_summaries -> all_summaries ë¡œ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼
    if 'fname' in input_df.columns:
        # ìµœì¢… ì œì¶œìš© test.csvì²˜ëŸ¼ fnameì´ ìˆëŠ” ê²½ìš°
        submission = pd.DataFrame({'fname': input_df['fname'], 'summary': all_summaries})
    else:
        # ì‹¤í—˜ìš© val.csvì²˜ëŸ¼ fnameì´ ì—†ëŠ” ê²½ìš°, ì„ì‹œë¡œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©
        submission = pd.DataFrame({'id': input_df.index, 'summary': all_summaries})
        print("âš ï¸ [ì •ë³´] ì…ë ¥ íŒŒì¼ì— 'fname' ì»¬ëŸ¼ì´ ì—†ì–´ ì„ì‹œ idë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

    os.makedirs("submissions", exist_ok=True)
    submission.to_csv(cfg.output_path, index=False, encoding='utf-8-sig')
    print(f"âœ… [ì„±ê³µ] ì¶”ë¡  ê²°ê³¼ê°€ '{cfg.output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # â–¼â–¼â–¼â–¼â–¼ ROUGE ê³„ì‚° ë¶€ë¶„ë„ english_summaries -> all_summaries ë¡œ ìˆ˜ì • â–¼â–¼â–¼â–¼â–¼
    if cfg.calculate_rouge:
        if 'english_summary' not in input_df.columns:
            print("âš ï¸ 'english_summary' ì»¬ëŸ¼ì´ ì—†ì–´ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        print("ğŸ“Š [í‰ê°€] ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...")
        rouge_metric = evaluate.load("rouge")
        references = input_df['english_summary'].tolist()
        results = rouge_metric.compute(predictions=all_summaries, references=references) # <-- ì—¬ê¸°ë„ ìˆ˜ì •!
        
        print("\n--- ğŸ“ ìµœì¢… ROUGE ì ìˆ˜ ğŸ“ ---")
        for key, value in results.items():
            print(f"- {key}: {value:.4f}")
        print("------------------------------")

# --- 4. í—¬í¼ í•¨ìˆ˜ë“¤ ---
def find_best_checkpoint(model_name):
    search_dirs = [
        os.path.join(os.getcwd(), "all_checkpoints", model_name),
        os.path.join(os.getcwd(), "all_checkpoints_backup", model_name)
    ]
    ckpt_files = []
    for s_dir in search_dirs: ckpt_files.extend(glob.glob(os.path.join(s_dir, "model-*.ckpt")))
    if not ckpt_files: raise FileNotFoundError(f"'{model_name}' ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    def get_score(p):
        match = re.search(r"rougeL=([\d.]+)", p)
        return float(match.group(1).rstrip('.')) if match else 0
    return max(ckpt_files, key=get_score)

# --- 5. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, required=True)
    parser.add_argument("--input_path", type=str, default="data/processed/val.csv")
    parser.add_argument("--output_path", type=str, default=f"submissions/temp_submission_{int(time.time())}.csv")
    parser.add_argument("--calculate_rouge", action='store_true')
    parser.add_argument("--num_beams", type=int, default=None)
    parser.add_argument("--length_penalty", type=float, default=None)
    parser.add_argument("--repetition_penalty", type=float, default=None)
    parser.add_argument("--temperature", type=float, default=None)
    parser.add_argument("--top_p", type=float, default=None)
    args, unknown_args = parser.parse_known_args()
    cfg = OmegaConf.load('configs/config.yaml')
    model_cfg = OmegaConf.load(f'configs/model/{args.model_name}.yaml')
    data_cfg = OmegaConf.load(f"configs/data/default.yaml")
    generation_cfg = OmegaConf.from_dotlist(unknown_args)
    arg_gen_cfg = {k: v for k, v in vars(args).items() if k not in ['model_name', 'input_path', 'output_path', 'calculate_rouge']}
    for k, v in arg_gen_cfg.items():
        if v is not None: OmegaConf.update(generation_cfg, k, v)
    cfg.merge_with({'model': model_cfg, 'data': data_cfg, 'generation': generation_cfg})
    cfg.input_path = args.input_path
    cfg.output_path = args.output_path
    cfg.calculate_rouge = args.calculate_rouge
    inference(cfg)