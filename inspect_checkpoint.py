# inspect_checkpoint.py
import torch
import sys

def inspect_ckpt(ckpt_path):
    print(f"π•µοΈβ€β™‚οΈ μ²΄ν¬ν¬μΈνΈ νμΌμ„ λ¶„μ„ν•©λ‹λ‹¤: {ckpt_path}")
    try:
        # CPUλ¥Ό μ‚¬μ©ν•΄μ„ μ•μ „ν•κ² νμΌμ„ λ΅λ“
        ckpt = torch.load(ckpt_path, map_location="cpu")
        
        # 1. μ–΄ν μ‚¬μ „ ν¬κΈ° ν™•μΈ
        embedding_shape = ckpt['state_dict']['model.base_model.model.shared.weight'].shape
        vocab_size = embedding_shape[0]
        print(f"β… μ–΄ν μ‚¬μ „ ν¬κΈ° (Vocabulary size): {vocab_size}")

        # 2. μ €μ¥λ ν•μ΄νΌνλΌλ―Έν„° ν™•μΈ
        if 'hyper_parameters' in ckpt and 'model_cfg' in ckpt['hyper_parameters']:
            hparams = ckpt['hyper_parameters']['model_cfg']
            model_name = hparams.get('pretrained_model_name_or_path', 'N/A')
            print(f"β… μ €μ¥λ κΈ°λ³Έ λ¨λΈ μ΄λ¦„: {model_name}")

            if vocab_size == 32128 and model_name == "google/flan-t5-large":
                print("\nπ‰π‰π‰ μ™„λ²½ν•©λ‹λ‹¤! μ΄ μ²΄ν¬ν¬μΈνΈλ” 'flan-t5-large'μ™€ 100% νΈν™λ©λ‹λ‹¤.")
            else:
                print("\nπ”¥π”¥π”¥ κ²½κ³ ! μ²΄ν¬ν¬μΈνΈ μ •λ³΄κ°€ μμƒκ³Ό λ‹¤λ¦…λ‹λ‹¤.")
        else:
            print("β οΈ μ²΄ν¬ν¬μΈνΈμ— ν•μ΄νΌνλΌλ―Έν„° μ •λ³΄κ°€ μ—†μµλ‹λ‹¤.")
            
    except Exception as e:
        print(f"π¨ μ²΄ν¬ν¬μΈνΈ λ¶„μ„ μ¤‘ μ—λ¬ λ°μƒ: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("μ‚¬μ©λ²•: python inspect_checkpoint.py <μ²΄ν¬ν¬μΈνΈ_νμΌ_κ²½λ΅>")
    else:
        inspect_ckpt(sys.argv[1])